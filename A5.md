# 머신러닝 시스템의 테스트와 검증법

인공지능 모델을 만든 후 이 모델이 잘 작동하는지 알기 위해서는 새로운 데이터셋을 시스템에 넣어보는 수 밖에 없습니다. 이를 공짜 점심 이론이라고도 하는데, 이를 위해 데이터셋을 훈련 세트와 테스트 세트로 나누는 방법 등을 적어보고자 합니다.
<br/><br/><br/>

## 공짜 점심 없음 이론(David Wolperts)
> 데이터에 관해 완벽하게 어떤 가정도 하지 않으면 특정한 모델을 선호할 근가 없음(경험하기 전에 더 잘 맞을 거라고 보장할 수 있는 모델이 없음)
<br/>
따라서 Field에서는 데이터에 관해 타당한 가정을 하고 적절한 모델을 평가합니다.
<br/><br/><br/>

## 훈련 세트와 테스트 세트로 나눕니다.
새로운 샘플에 대한 오차율을 **일반화 오차(외부 샘플 오차)** 라고 하며, 테스트 세트에서 모델을 평가함으로써, 모델이 얼마나 잘 작동하는지 보여줍니다.
#### 만약 훈련 오차가 낮지만, 일반화 오차가 높다면 이는 모델이 훈련 데이터에 과대적합되었다는 뜻입니다.
#### 보통 데이터의 80%를 훈련에 사용하고 20%는 테스트용으로 떼어놓습니다.
<br/><br/><br/>

## 검증 세트를 만듭니다.
### 검증 세트가 필요한 이유
만약 인공지능 모델을 2개 만들었고, 이 2개 모델 중 어떤 모델이 적합한지 테스트 하고 싶다고 가정합시다.<br/>
두 모델 모두 훈련 세트로 훈련시키고 테스트 세트를 통해 얼마나 잘 일반화 되었는지 비교해봅니다.<br/>
하지만 정말 오차가 작아도 실제 서비스에 투입하면 오차가 많이 나올 수도 있습니다.
그 이유는, 일반화 오차를 테스트 세트에서만 측정 했으므로, 모델과 하이퍼파라미터가 테스트 세트에 최적화된 모델을 만들었기 때문입니다.
<hr/>

### 방법
보통 훈련 세트를 사용해 다양한 하이퍼파라미터로 여러 모델을 훈련시키고 검증 세트에서 최상의 성능을 내는 모델과 하이퍼파라미터를 선택하게 됩니다. 모델에 만족하면, 일반화 오차의 추정값을 얻기 위해 테스트 세트로 최종 테스트를 하게 됩니다.
<hr/>

### 교차 검증
데이터를 훈련하면서 검증 세트로 너무 많은 양의 데이터를 뺏기지 않기 위해 교차 검증이라는 기법을 사용합니다.<br/>
훈련 세트를 여러 서브넷으로 나누고 각 모델을 이 서브넷의 조합으로 훈련시키고 나머지 부분으로 검증하는 것입니다.<br/>
모델과 하이퍼파라미터가 선택되면 전체 훈련 데이터를 사용하여 선택한 하이퍼파라미터로 최종 모델을 훈련시키고 테스트 세트에서 일반화 오차를 측정합니다.
<hr/>
